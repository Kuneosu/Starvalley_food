#!/usr/bin/env node

import { Command } from 'commander';
import chalk from 'chalk';
import { scrapeMenu } from '../src/scraper.js';
import { extractTextFromImage } from '../src/ocr.js';
import { extractTextWithClientOCR, extractTextWithEnhancedTesseract } from '../src/easyocr.js';
import { extractMenuWithVision, checkOllamaStatus } from '../src/vision.js';
import { hybridOCRAI, processOCRWithAI, checkAIStatus } from '../src/textProcessor.js';
import { parseMenu } from '../src/parser.js';

const program = new Command();

program
  .name('st-food')
  .description('Star Valley cafeteria menu CLI tool')
  .version('1.0.0');

program
  .command('today')
  .description('Get today\'s menu using AI-enhanced processing')
  .option('--ocr', 'Use legacy Tesseract OCR only')
  .option('--easyocr', 'Use enhanced OCR only')
  .option('--advanced-ocr', 'Use advanced client-side OCR only')
  .option('--vision', 'Use LLaVA vision model only')
  .option('--hybrid', 'Use OCR + AI text processing (recommended)')
  .option('--no-ai', 'Skip AI text processing')
  .action(async (options) => {
    try {
      console.log(chalk.blue('üçΩÔ∏è  Fetching Star Valley cafeteria menu...'));
      
      const imageUrl = await scrapeMenu();
      
      let extractedText;
      
      // ÌïòÏù¥Î∏åÎ¶¨Îìú Î™®Îìú (Í∏∞Î≥∏ Í∂åÏû•)
      if (options.hybrid || (!options.ocr && !options.easyocr && !options.advancedOcr && !options.vision && !options.noAi)) {
        console.log(chalk.cyan('üöÄ Using Hybrid OCR + AI processing (recommended)...'));
        extractedText = await hybridOCRAI(imageUrl, extractTextFromImage);
      }
      // Î™ÖÏãúÏ†Å ÏòµÏÖòÎì§
      else if (options.vision) {
        console.log(chalk.yellow('Using LLaVA vision model...'));
        extractedText = await extractMenuWithVision(imageUrl);
      } else if (options.ocr) {
        if (options.noAi) {
          console.log(chalk.yellow('Using legacy OCR only...'));
          extractedText = parseMenu(await extractTextFromImage(imageUrl));
        } else {
          console.log(chalk.cyan('Using legacy OCR + AI processing...'));
          const ocrText = await extractTextFromImage(imageUrl);
          extractedText = await processOCRWithAI(parseMenu(ocrText));
        }
      } else if (options.easyocr) {
        if (options.noAi) {
          console.log(chalk.yellow('Using enhanced OCR only...'));
          extractedText = await extractTextWithEnhancedTesseract(imageUrl);
        } else {
          console.log(chalk.cyan('Using enhanced OCR + AI processing...'));
          const ocrText = await extractTextWithEnhancedTesseract(imageUrl);
          extractedText = await processOCRWithAI(ocrText);
        }
      } else if (options.advancedOcr) {
        if (options.noAi) {
          console.log(chalk.yellow('Using advanced OCR only...'));
          extractedText = await extractTextWithClientOCR(imageUrl);
        } else {
          console.log(chalk.cyan('Using advanced OCR + AI processing...'));
          const ocrText = await extractTextWithClientOCR(imageUrl);
          extractedText = await processOCRWithAI(ocrText);
        }
      } else {
        // ÏûêÎèô Î™®Îìú: AI ÌõÑÏ≤òÎ¶¨ Ìè¨Ìï®
        const ollamaRunning = await checkOllamaStatus();
        
        if (!ollamaRunning) {
          console.log(chalk.yellow('‚ö†Ô∏è  Ollama server not running. Using OCR only mode...'));
          extractedText = parseMenu(await extractTextFromImage(imageUrl));
        } else {
          console.log(chalk.cyan('üöÄ Using auto hybrid mode...'));
          extractedText = await hybridOCRAI(imageUrl, extractTextFromImage);
        }
      }
      
      console.log(chalk.green('\nüìã Today\'s Menu:'));
      console.log(extractedText);
      
    } catch (error) {
      console.error(chalk.red('‚ùå Error:'), error.message);
      
      // ÏóêÎü¨ Î∞úÏÉùÏãú ÎèÑÏõÄÎßê ÌëúÏãú
      if (error.message.includes('Ollama')) {
        console.log(chalk.yellow('\nüí° Tip: Make sure Ollama is installed and running:'));
        console.log(chalk.cyan('  1. Install Ollama: https://ollama.ai'));
        console.log(chalk.cyan('  2. Start server: ollama serve'));
        console.log(chalk.cyan('  3. Pull model: ollama pull llava:7b'));
      }
      
      if (error.message.includes('OCR') || error.message.includes('AI')) {
        console.log(chalk.yellow('\nüí° Available Options:'));
        console.log(chalk.green('  (default)       Hybrid OCR + AI processing'));
        console.log(chalk.cyan('  --hybrid        Explicitly use OCR + AI'));
        console.log(chalk.cyan('  --ocr           Legacy OCR (+ AI if available)'));
        console.log(chalk.cyan('  --vision        LLaVA vision model'));
        console.log(chalk.cyan('  --no-ai         Skip AI text processing'));
      }
      
      process.exit(1);
    }
  });

program
  .command('raw')
  .description('Get raw text extraction for debugging')
  .option('--ocr', 'Force legacy Tesseract OCR')
  .option('--easyocr', 'Force enhanced Tesseract OCR')
  .option('--advanced-ocr', 'Force advanced client-side OCR')
  .option('--vision', 'Force LLaVA vision model')
  .action(async (options) => {
    try {
      console.log(chalk.blue('üîç Extracting raw text from menu image...'));
      
      const imageUrl = await scrapeMenu();
      let extractedText;
      
      if (options.ocr) {
        console.log(chalk.yellow('Using legacy Tesseract OCR...'));
        extractedText = await extractTextFromImage(imageUrl);
      } else if (options.easyocr) {
        console.log(chalk.yellow('Using enhanced Tesseract OCR...'));
        extractedText = await extractTextWithEnhancedTesseract(imageUrl);
      } else if (options.advancedOcr) {
        console.log(chalk.yellow('Using advanced client-side OCR...'));
        extractedText = await extractTextWithClientOCR(imageUrl);
      } else if (options.vision) {
        console.log(chalk.yellow('Using LLaVA vision model...'));
        extractedText = await extractMenuWithVision(imageUrl);
      } else {
        // Í∏∞Î≥∏Í∞í: Ollama ÏÇ¨Ïö© Í∞ÄÎä•ÌïòÎ©¥ vision, ÏïÑÎãàÎ©¥ enhanced OCR
        const ollamaRunning = await checkOllamaStatus();
        
        if (ollamaRunning) {
          console.log(chalk.yellow('Auto: Using LLaVA vision model...'));
          extractedText = await extractMenuWithVision(imageUrl);
        } else {
          console.log(chalk.yellow('Auto: Ollama not running, trying enhanced OCR...'));
          
          try {
            extractedText = await extractTextWithEnhancedTesseract(imageUrl);
          } catch (enhancedError) {
            console.log(chalk.yellow('Enhanced OCR failed, using legacy OCR...'));
            extractedText = await extractTextFromImage(imageUrl);
          }
        }
      }
      
      console.log(chalk.yellow('\nüìÑ Raw Extracted Text:'));
      console.log(extractedText);
      
    } catch (error) {
      console.error(chalk.red('‚ùå Error:'), error.message);
      process.exit(1);
    }
  });

program.parse();